{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('mnist_train.csv',delimiter=',',dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = data[:,1:]\n",
    "y_train = data[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = (np.arange(10) == y_train[:,None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_size=500\n",
    "IMAGE_SIZE = 28\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val, x_train = x_train[:validation_size, :], x_train[validation_size:,:]\n",
    "y_val, y_train = y_train[:validation_size, :], y_train[validation_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(len(x_train),IMAGE_SIZE, IMAGE_SIZE,1)\n",
    "x_val = x_val.reshape(len(x_val), IMAGE_SIZE, IMAGE_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,batch_size=128, learning_rate=1e-4, num_labels=10):\n",
    "        self._batch_size=batch_size\n",
    "        self._learning_rate=learning_rate\n",
    "        self._num_labels = num_labels\n",
    "        \n",
    "    def _create_conv2d(self,x,w):\n",
    "        return tf.nn.conv2d(input=x, filter=w, strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    def _create_maxpool(self,x):\n",
    "        return tf.nn.max_pool(value=x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    def _create_weights(self,shape):\n",
    "        return tf.Variable(initial_value=tf.truncated_normal(shape=shape, stddev=0.1, dtype=tf.float32))\n",
    "    \n",
    "    def _create_biases(self,shape):\n",
    "        return tf.Variable(initial_value=tf.constant(1., shape=shape, dtype=tf.float32))\n",
    "    \n",
    "    def _activation_summary(self,x):\n",
    "        tensor_name = x.op.name\n",
    "        tf.summary.histogram(tensor_name+'/activations',x)\n",
    "        tf.summary.scalar(tensor_name+'/sparsity',tf.nn.zero_fraction(x))\n",
    "        \n",
    "    def inference(self, image, keep_prob):\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            kernel = self._create_weights([5,5,1,32])\n",
    "            conv = self._create_conv2d(image,kernel)\n",
    "            bias = self._create_biases([32])\n",
    "            preactivation = tf.nn.bias_add(value=conv, bias=bias)\n",
    "            conv1 = tf.nn.relu(preactivation, name=scope.name)\n",
    "            self._activation_summary(conv1)\n",
    "        \n",
    "        pool1 = self._create_maxpool(conv1)\n",
    "        \n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            kernel = self._create_weights([5,5,32,64])\n",
    "            conv = self._create_conv2d(pool1,kernel)\n",
    "            bias = self._create_biases([64])\n",
    "            preactivation = tf.nn.bias_add(value=conv, bias=bias)\n",
    "            conv2 = tf.nn.relu(preactivation, name=scope.name)\n",
    "            self._activation_summary(conv2)\n",
    "            \n",
    "        pool2 = self._create_maxpool(conv2)\n",
    "        \n",
    "        with tf.variable_scope('local1') as scope:\n",
    "            reshape = tf.reshape(pool2, shape=[-1,7*7*64])\n",
    "            weights = self._create_weights([7*7*64,1024])\n",
    "            biases = self._create_biases([1024])\n",
    "            local1 = tf.nn.relu(tf.matmul(reshape,weights)+biases, name=scope.name)\n",
    "            self._activation_summary(local1)\n",
    "            \n",
    "        with tf.variable_scope('local2_linear') as scope:\n",
    "            W_fc2 = self._create_weights([1024, self._num_labels])\n",
    "            b_fc2 = self._create_biases([self._num_labels])\n",
    "            local1_drop = tf.nn.dropout(local1, keep_prob)\n",
    "            local2 = tf.nn.bias_add(tf.matmul(local1_drop, W_fc2), b_fc2, name=scope.name)\n",
    "            self._activation_summary(local2)\n",
    "        return local2\n",
    "    \n",
    "    def train(self,loss,global_step):\n",
    "        tf.summary.scalar('learning_rate',self._learning_rate)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=self._learning_rate).minimize(loss, global_step=global_step)\n",
    "        \n",
    "    def loss(self,logits, labels):\n",
    "        \n",
    "        with tf.variable_scope('loss') as scope:\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "            cost = tf.reduce_mean(cross_entropy,name=scope.name)\n",
    "        return cost\n",
    "    \n",
    "    def accuracy(self,logits,labels):\n",
    "        with tf.variable_scope('accuracy') as scope:\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits,1),tf.argmax(labels,1)),dtype=tf.float32),name=scope.name)\n",
    "            tf.summary.scalar('accuracy',accuracy)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "NUM_LABELS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-9a7fc27cbd18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lei\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lei\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1107\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1109\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lei\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \"\"\"\n\u001b[0;32m    412\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lei\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lei\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \"\"\"\n\u001b[0;32m    339\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lei\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \"\"\"\n\u001b[0;32m    339\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lei\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' %\n\u001b[1;32m--> 230\u001b[1;33m                       (fetch, type(fetch)))\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    x=tf.placeholder(shape=[None,IMAGE_SIZE, IMAGE_SIZE, 1], dtype=tf.float32,name='x')\n",
    "    y=tf.placeholder(shape=[None, NUM_LABELS], dtype=tf.float32,name='y')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='dropout_prob')\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    \n",
    "    logits = model.inference(x,keep_prob)\n",
    "    loss=model.loss(labels=y,logits=logits)\n",
    "    \n",
    "    accuracy = model.accuracy(logits,y)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    train_op = model.train(loss=loss,global_step=global_step)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        writer = tf.summary.FileWriter('cache',sess.graph)\n",
    "        sess.run(init)\n",
    "        \n",
    "        for i in range(1000):\n",
    "            offset = (i*batch_size)%len(x_train)\n",
    "            batch_x,batch_y = x_train[offset:(offset + batch_size),:], y_train[offset:(offset+batch_size),:]\n",
    "            _, cur_loss, summary = sess.run([train_op,loss,summary_op], feed_dict={x:batch_x, y:batch_y,keep_prob:0.5})\n",
    "            writer.add_summary(summary,i)\n",
    "            \n",
    "            print(i,cur_loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2134.8\n",
      "1 2032.85\n",
      "2 1819.03\n",
      "3 1688.28\n",
      "4 1242.32\n",
      "5 1528.5\n",
      "6 1406.81\n",
      "7 1269.45\n",
      "8 1312.89\n",
      "9 1247.7\n",
      "10 1092.42\n",
      "11 1114.93\n",
      "12 1064.75\n",
      "13 1093.43\n",
      "14 889.244\n",
      "15 899.378\n",
      "16 837.318\n",
      "17 879.899\n",
      "18 770.422\n",
      "19 704.172\n",
      "20 733.14\n",
      "21 615.44\n",
      "22 670.869\n",
      "23 665.734\n",
      "24 569.871\n",
      "25 613.447\n",
      "26 531.629\n",
      "27 464.418\n",
      "28 519.512\n",
      "29 494.999\n",
      "30 374.483\n",
      "31 397.567\n",
      "32 455.774\n",
      "33 383.197\n",
      "34 470.129\n",
      "35 383.937\n",
      "36 346.264\n",
      "37 359.638\n",
      "38 362.922\n",
      "39 275.325\n",
      "40 345.422\n",
      "41 406.787\n",
      "42 315.568\n",
      "43 332.128\n",
      "44 231.965\n",
      "45 269.014\n",
      "46 256.665\n",
      "47 205.726\n",
      "48 215.366\n",
      "49 372.034\n",
      "50 251.35\n",
      "51 235.343\n",
      "52 328.792\n",
      "53 241.216\n",
      "54 240.073\n",
      "55 196.993\n",
      "56 237.431\n",
      "57 208.84\n",
      "58 233.943\n",
      "59 198.599\n",
      "60 255.531\n",
      "61 186.335\n",
      "62 235.61\n",
      "63 216.283\n",
      "64 264.548\n",
      "65 255.082\n",
      "66 126.233\n",
      "67 135.789\n",
      "68 205.229\n",
      "69 206.132\n",
      "70 162.709\n",
      "71 191.091\n",
      "72 160.559\n",
      "73 133.172\n",
      "74 119.394\n",
      "75 200.535\n",
      "76 176.546\n",
      "77 127.382\n",
      "78 129.023\n",
      "79 126.062\n",
      "80 169.281\n",
      "81 149.082\n",
      "82 149.935\n",
      "83 165.947\n",
      "84 66.3887\n",
      "85 121.488\n",
      "86 150.788\n",
      "87 164.87\n",
      "88 161.357\n",
      "89 116.819\n",
      "90 86.7071\n",
      "91 124.558\n",
      "92 176.625\n",
      "93 131.505\n",
      "94 167.138\n",
      "95 152.701\n",
      "96 79.9475\n",
      "97 188.209\n",
      "98 159.601\n",
      "99 138.906\n",
      "100 99.4009\n",
      "101 120.776\n",
      "102 96.4964\n",
      "103 131.054\n",
      "104 130.328\n",
      "105 135.791\n",
      "106 160.866\n",
      "107 106.79\n",
      "108 135.594\n",
      "109 161.091\n",
      "110 169.671\n",
      "111 195.486\n",
      "112 60.4824\n",
      "113 77.6609\n",
      "114 130.958\n",
      "115 59.9466\n",
      "116 106.101\n",
      "117 93.5874\n",
      "118 78.5276\n",
      "119 149.274\n",
      "120 108.843\n",
      "121 115.514\n",
      "122 92.471\n",
      "123 71.3438\n",
      "124 83.1029\n",
      "125 107.358\n",
      "126 105.299\n",
      "127 96.9435\n",
      "128 96.5292\n",
      "129 85.5303\n",
      "130 80.5671\n",
      "131 52.8215\n",
      "132 95.0715\n",
      "133 86.0903\n",
      "134 109.461\n",
      "135 118.465\n",
      "136 34.7945\n",
      "137 62.996\n",
      "138 48.3327\n",
      "139 65.696\n",
      "140 95.2447\n",
      "141 89.6621\n",
      "142 79.0258\n",
      "143 64.4443\n",
      "144 40.5866\n",
      "145 80.2582\n",
      "146 91.203\n",
      "147 52.6783\n",
      "148 62.3865\n",
      "149 78.6322\n",
      "150 49.463\n",
      "151 102.25\n",
      "152 69.3526\n",
      "153 88.9919\n",
      "154 51.3221\n",
      "155 61.627\n",
      "156 71.4\n",
      "157 68.4352\n",
      "158 73.6521\n",
      "159 104.326\n",
      "160 55.1493\n",
      "161 54.509\n",
      "162 78.3589\n",
      "163 57.0869\n",
      "164 68.0335\n",
      "165 35.4524\n",
      "166 42.7594\n",
      "167 64.7902\n",
      "168 87.3942\n",
      "169 123.42\n",
      "170 40.0031\n",
      "171 86.7778\n",
      "172 137.044\n",
      "173 56.516\n",
      "174 51.1148\n",
      "175 42.7109\n",
      "176 63.7577\n",
      "177 35.6493\n",
      "178 62.5997\n",
      "179 68.6291\n",
      "180 91.494\n",
      "181 74.4984\n",
      "182 47.0853\n",
      "183 53.2311\n",
      "184 29.3438\n",
      "185 83.4226\n",
      "186 54.996\n",
      "187 56.3967\n",
      "188 68.732\n",
      "189 74.7488\n",
      "190 61.1367\n",
      "191 39.9143\n",
      "192 64.2766\n",
      "193 89.0859\n",
      "194 51.9617\n",
      "195 41.0672\n",
      "196 39.97\n",
      "197 71.4588\n",
      "198 51.8746\n",
      "199 25.0859\n",
      "200 62.6004\n",
      "201 35.8377\n",
      "202 98.034\n",
      "203 69.2011\n",
      "204 92.497\n",
      "205 55.8876\n",
      "206 42.372\n",
      "207 45.9486\n",
      "208 52.8502\n",
      "209 30.8578\n",
      "210 69.1514\n",
      "211 38.1676\n",
      "212 57.1349\n",
      "213 57.6752\n",
      "214 44.2771\n",
      "215 44.9583\n",
      "216 63.477\n",
      "217 54.3405\n",
      "218 51.4873\n",
      "219 69.2434\n",
      "220 39.4981\n",
      "221 33.612\n",
      "222 45.587\n",
      "223 47.8368\n",
      "224 64.0952\n",
      "225 57.6926\n",
      "226 70.1958\n",
      "227 49.0019\n",
      "228 54.7448\n",
      "229 61.7366\n",
      "230 55.4532\n",
      "231 48.9139\n",
      "232 31.7168\n",
      "233 43.1072\n",
      "234 59.9143\n",
      "235 68.4364\n",
      "236 50.5885\n",
      "237 57.4981\n",
      "238 79.0354\n",
      "239 62.4866\n",
      "240 46.7937\n",
      "241 45.3066\n",
      "242 56.9649\n",
      "243 70.4418\n",
      "244 34.3158\n",
      "245 31.7609\n",
      "246 50.1266\n",
      "247 66.5341\n",
      "248 77.3141\n",
      "249 54.1071\n",
      "250 29.4936\n",
      "251 39.5493\n",
      "252 55.3803\n",
      "253 70.0972\n",
      "254 51.0579\n",
      "255 58.1692\n",
      "256 71.4936\n",
      "257 33.2216\n",
      "258 24.8651\n",
      "259 40.6196\n",
      "260 24.6649\n",
      "261 33.0672\n",
      "262 38.9385\n",
      "263 18.3743\n",
      "264 39.9952\n",
      "265 52.2452\n",
      "266 59.4477\n",
      "267 44.2627\n",
      "268 67.1536\n",
      "269 28.5198\n",
      "270 22.2518\n",
      "271 33.7872\n",
      "272 23.6246\n",
      "273 53.4102\n",
      "274 54.8153\n",
      "275 25.4449\n",
      "276 35.5377\n",
      "277 45.0201\n",
      "278 33.1553\n",
      "279 42.5002\n",
      "280 50.7609\n",
      "281 46.5943\n",
      "282 25.315\n",
      "283 49.872\n",
      "284 47.6491\n",
      "285 50.9246\n",
      "286 54.9735\n",
      "287 58.6269\n",
      "288 84.8135\n",
      "289 52.0519\n",
      "290 26.4212\n",
      "291 61.4619\n",
      "292 29.4832\n",
      "293 29.6207\n",
      "294 35.435\n",
      "295 40.2585\n",
      "296 24.667\n",
      "297 45.4654\n",
      "298 31.1755\n",
      "299 34.2735\n",
      "300 32.2484\n",
      "301 18.0894\n",
      "302 24.8041\n",
      "303 76.5442\n",
      "304 41.6552\n",
      "305 29.4695\n",
      "306 37.844\n",
      "307 45.2765\n",
      "308 45.6451\n",
      "309 17.5713\n",
      "310 19.1396\n",
      "311 16.3856\n",
      "312 26.5438\n",
      "313 32.6424\n",
      "314 23.7311\n",
      "315 16.5288\n",
      "316 32.9319\n",
      "317 24.808\n",
      "318 57.4457\n",
      "319 61.0102\n",
      "320 35.9479\n",
      "321 32.5042\n",
      "322 40.3857\n",
      "323 35.2402\n",
      "324 24.4233\n",
      "325 44.4711\n",
      "326 30.6437\n",
      "327 37.5033\n",
      "328 49.9938\n",
      "329 41.9743\n",
      "330 41.4056\n",
      "331 42.8404\n",
      "332 37.0884\n",
      "333 36.94\n",
      "334 29.3915\n",
      "335 17.8541\n",
      "336 33.9455\n",
      "337 38.8254\n",
      "338 33.3685\n",
      "339 44.5497\n",
      "340 51.2875\n",
      "341 40.2357\n",
      "342 31.6034\n",
      "343 35.0875\n",
      "344 23.3801\n",
      "345 38.937\n",
      "346 44.6154\n",
      "347 37.6936\n",
      "348 32.3466\n",
      "349 21.9363\n",
      "350 12.1046\n",
      "351 65.157\n",
      "352 32.156\n",
      "353 34.8409\n",
      "354 49.9607\n",
      "355 38.7908\n",
      "356 38.2071\n",
      "357 41.2795\n",
      "358 33.8311\n",
      "359 15.244\n",
      "360 44.4572\n",
      "361 17.5307\n",
      "362 20.2451\n",
      "363 18.646\n",
      "364 16.9705\n",
      "365 54.6012\n",
      "366 29.22\n",
      "367 33.3306\n",
      "368 14.3495\n",
      "369 20.3454\n",
      "370 53.2196\n",
      "371 26.2252\n",
      "372 20.1788\n",
      "373 44.9456\n",
      "374 23.0083\n",
      "375 42.7392\n",
      "376 22.8293\n",
      "377 10.3986\n",
      "378 35.8677\n",
      "379 27.8273\n",
      "380 45.5677\n",
      "381 17.6227\n",
      "382 50.2497\n",
      "383 44.5491\n",
      "384 39.1699\n",
      "385 49.0262\n",
      "386 25.3755\n",
      "387 31.2523\n",
      "388 30.3218\n",
      "389 53.2338\n",
      "390 32.9594\n",
      "391 26.3966\n",
      "392 45.3897\n",
      "393 39.1485\n",
      "394 29.3582\n",
      "395 20.4036\n",
      "396 23.467\n",
      "397 25.1163\n",
      "398 19.6858\n",
      "399 17.7539\n",
      "400 24.8918\n",
      "401 27.8376\n",
      "402 28.2511\n",
      "403 71.1039\n",
      "404 44.379\n",
      "405 13.1327\n",
      "406 17.991\n",
      "407 26.9894\n",
      "408 34.7065\n",
      "409 46.0382\n",
      "410 23.0771\n",
      "411 15.6199\n",
      "412 11.1962\n",
      "413 20.8175\n",
      "414 30.8035\n",
      "415 23.0082\n",
      "416 29.7297\n",
      "417 28.3118\n",
      "418 28.6941\n",
      "419 23.8741\n",
      "420 11.2016\n",
      "421 22.5739\n",
      "422 18.9632\n",
      "423 9.78794\n",
      "424 17.6342\n",
      "425 35.1474\n",
      "426 12.8419\n",
      "427 12.3232\n",
      "428 16.7736\n",
      "429 21.3171\n",
      "430 18.6211\n",
      "431 26.6548\n",
      "432 26.1664\n",
      "433 5.37454\n",
      "434 21.9248\n",
      "435 32.2509\n",
      "436 15.83\n",
      "437 34.97\n",
      "438 18.3379\n",
      "439 10.4271\n",
      "440 24.0382\n",
      "441 12.7901\n",
      "442 7.29363\n",
      "443 25.3054\n",
      "444 29.895\n",
      "445 18.7979\n",
      "446 32.0083\n",
      "447 16.768\n",
      "448 11.6651\n",
      "449 21.2658\n",
      "450 6.98097\n",
      "451 7.1247\n",
      "452 8.8327\n",
      "453 7.7945\n",
      "454 7.55731\n",
      "455 12.7923\n",
      "456 6.55534\n",
      "457 17.855\n",
      "458 5.10133\n",
      "459 28.4683\n",
      "460 24.089\n",
      "461 4.40319\n",
      "462 45.0203\n",
      "463 9.68048\n",
      "464 13.3694\n",
      "465 29.6018\n",
      "466 29.3484\n",
      "467 10.7827\n",
      "468 38.9784\n",
      "469 26.3519\n",
      "470 37.831\n",
      "471 13.9973\n",
      "472 16.0692\n",
      "473 11.308\n",
      "474 8.89453\n",
      "475 21.6991\n",
      "476 13.7084\n",
      "477 12.9524\n",
      "478 21.303\n",
      "479 9.4179\n",
      "480 30.1819\n",
      "481 19.0414\n",
      "482 19.1027\n",
      "483 28.7926\n",
      "484 14.3874\n",
      "485 13.5275\n",
      "486 18.3888\n",
      "487 26.5538\n",
      "488 18.8967\n",
      "489 22.5048\n",
      "490 8.90987\n",
      "491 17.1687\n",
      "492 20.2881\n",
      "493 15.0278\n",
      "494 23.709\n",
      "495 21.1604\n",
      "496 23.8301\n",
      "497 34.7128\n",
      "498 23.986\n",
      "499 24.4589\n",
      "500 21.4642\n",
      "501 25.016\n",
      "502 16.4758\n",
      "503 22.7743\n",
      "504 23.4007\n",
      "505 25.3454\n",
      "506 17.084\n",
      "507 18.3652\n",
      "508 6.92299\n",
      "509 19.5457\n",
      "510 21.3697\n",
      "511 7.43926\n",
      "512 28.1259\n",
      "513 36.5969\n",
      "514 28.0811\n",
      "515 26.7196\n",
      "516 31.8885\n",
      "517 23.7065\n",
      "518 14.499\n",
      "519 14.6669\n",
      "520 19.5638\n",
      "521 15.8425\n",
      "522 21.054\n",
      "523 16.111\n",
      "524 26.7594\n",
      "525 7.60502\n",
      "526 32.9692\n",
      "527 29.8651\n",
      "528 31.4126\n",
      "529 43.1023\n",
      "530 10.709\n",
      "531 19.3068\n",
      "532 14.3679\n",
      "533 26.6198\n",
      "534 12.6149\n",
      "535 23.6837\n",
      "536 12.1474\n",
      "537 7.05875\n",
      "538 13.0494\n",
      "539 28.1916\n",
      "540 35.5316\n",
      "541 7.4898\n",
      "542 6.88735\n",
      "543 21.7058\n",
      "544 19.6679\n",
      "545 14.3232\n",
      "546 14.3983\n",
      "547 24.2133\n",
      "548 14.3071\n",
      "549 7.74181\n",
      "550 26.9376\n",
      "551 38.2577\n",
      "552 12.1403\n",
      "553 23.3994\n",
      "554 24.0385\n",
      "555 17.6896\n",
      "556 22.4173\n",
      "557 9.85556\n",
      "558 32.0\n",
      "559 22.4038\n",
      "560 20.5715\n",
      "561 19.5526\n",
      "562 32.1538\n",
      "563 3.53551\n",
      "564 11.5213\n",
      "565 11.9164\n",
      "566 19.648\n",
      "567 12.4696\n",
      "568 17.5477\n",
      "569 35.7318\n",
      "570 27.7485\n",
      "571 16.6196\n",
      "572 11.8758\n",
      "573 21.7452\n",
      "574 20.815\n",
      "575 53.9409\n",
      "576 13.1767\n",
      "577 12.1765\n",
      "578 16.1566\n",
      "579 24.4527\n",
      "580 19.1567\n",
      "581 6.85259\n",
      "582 18.5338\n",
      "583 30.2398\n",
      "584 19.4908\n",
      "585 15.249\n",
      "586 24.8894\n",
      "587 4.3261\n",
      "588 9.67013\n",
      "589 7.92322\n",
      "590 18.2604\n",
      "591 22.248\n",
      "592 21.3171\n",
      "593 16.1891\n",
      "594 32.87\n",
      "595 16.3603\n",
      "596 19.7228\n",
      "597 21.9048\n",
      "598 23.6172\n",
      "599 15.8777\n",
      "600 8.90331\n",
      "601 11.6282\n",
      "602 7.18778\n",
      "603 19.0906\n",
      "604 13.186\n",
      "605 12.7688\n",
      "606 15.9667\n",
      "607 6.58767\n",
      "608 3.60389\n",
      "609 15.2714\n",
      "610 19.4093\n",
      "611 16.2715\n",
      "612 22.5053\n",
      "613 3.72911\n",
      "614 15.4238\n",
      "615 9.39762\n",
      "616 9.70693\n",
      "617 29.0115\n",
      "618 19.2016\n",
      "619 4.07825\n",
      "620 13.1633\n",
      "621 23.765\n",
      "622 20.0896\n",
      "623 14.2207\n",
      "624 12.4547\n",
      "625 11.4203\n",
      "626 11.3185\n",
      "627 13.8047\n",
      "628 10.6974\n",
      "629 10.0904\n",
      "630 3.37048\n",
      "631 14.5148\n",
      "632 14.3252\n",
      "633 12.9281\n",
      "634 7.56776\n",
      "635 28.3242\n",
      "636 36.1588\n",
      "637 14.526\n",
      "638 10.4044\n",
      "639 5.01192\n",
      "640 10.9764\n",
      "641 14.3056\n",
      "642 13.9282\n",
      "643 11.8117\n",
      "644 19.1464\n",
      "645 10.9636\n",
      "646 10.081\n",
      "647 25.0083\n",
      "648 11.4263\n",
      "649 14.2334\n",
      "650 4.95687\n",
      "651 25.8784\n",
      "652 19.3671\n",
      "653 18.9625\n",
      "654 21.0043\n",
      "655 12.0649\n",
      "656 10.8179\n",
      "657 16.827\n",
      "658 7.36772\n",
      "659 15.5058\n",
      "660 10.4949\n",
      "661 18.2803\n",
      "662 16.4669\n",
      "663 3.98384\n",
      "664 16.7776\n",
      "665 8.62347\n",
      "666 17.9653\n",
      "667 19.6145\n",
      "668 23.0446\n",
      "669 16.9543\n",
      "670 7.55378\n",
      "671 27.567\n",
      "672 22.2659\n",
      "673 12.2508\n",
      "674 20.2339\n",
      "675 8.30361\n",
      "676 20.7037\n",
      "677 19.0976\n",
      "678 13.2867\n",
      "679 8.30262\n",
      "680 14.8615\n",
      "681 24.2717\n",
      "682 8.21631\n",
      "683 18.4135\n",
      "684 19.6047\n",
      "685 11.5254\n",
      "686 20.8067\n",
      "687 16.6013\n",
      "688 10.4116\n",
      "689 16.8869\n",
      "690 9.652\n",
      "691 4.99318\n",
      "692 11.6497\n",
      "693 25.2514\n",
      "694 22.363\n",
      "695 15.8623\n",
      "696 3.43576\n",
      "697 13.1149\n",
      "698 15.5035\n",
      "699 9.68661\n",
      "700 18.8093\n",
      "701 17.7143\n",
      "702 15.2743\n",
      "703 26.1599\n",
      "704 20.2164\n",
      "705 8.46665\n",
      "706 15.4552\n",
      "707 24.2021\n",
      "708 15.2123\n",
      "709 6.17773\n",
      "710 11.498\n",
      "711 9.93452\n",
      "712 5.78774\n",
      "713 21.0951\n",
      "714 6.58038\n",
      "715 12.619\n",
      "716 20.7311\n",
      "717 12.9086\n",
      "718 15.4832\n",
      "719 9.86087\n",
      "720 13.2372\n",
      "721 7.88107\n",
      "722 13.4337\n",
      "723 16.264\n",
      "724 8.92665\n",
      "725 15.4975\n",
      "726 5.60496\n",
      "727 6.44277\n",
      "728 12.5276\n",
      "729 11.0381\n",
      "730 17.098\n",
      "731 17.228\n",
      "732 12.8333\n",
      "733 9.12124\n",
      "734 11.1056\n",
      "735 8.00635\n",
      "736 8.7138\n",
      "737 14.1622\n",
      "738 14.099\n",
      "739 8.62902\n",
      "740 12.3824\n",
      "741 8.13949\n",
      "742 10.1166\n",
      "743 10.1708\n",
      "744 16.9828\n",
      "745 8.03019\n",
      "746 12.4574\n",
      "747 23.6375\n",
      "748 5.87602\n",
      "749 21.1221\n",
      "750 12.973\n",
      "751 20.1686\n",
      "752 22.5171\n",
      "753 9.10678\n",
      "754 10.4655\n",
      "755 14.3424\n",
      "756 9.24226\n",
      "757 5.31833\n",
      "758 3.96778\n",
      "759 11.8113\n",
      "760 6.84324\n",
      "761 14.6994\n",
      "762 14.3659\n",
      "763 12.0007\n",
      "764 9.85313\n",
      "765 9.2228\n",
      "766 9.19597\n",
      "767 33.6507\n",
      "768 17.8656\n",
      "769 11.8366\n",
      "770 8.46177\n",
      "771 17.146\n",
      "772 3.29624\n",
      "773 12.6423\n",
      "774 8.09029\n",
      "775 6.8572\n",
      "776 12.3507\n",
      "777 12.7225\n",
      "778 8.17728\n",
      "779 7.97059\n",
      "780 17.5875\n",
      "781 11.613\n",
      "782 26.28\n",
      "783 20.9609\n",
      "784 14.9786\n",
      "785 7.48004\n",
      "786 7.43448\n",
      "787 16.2217\n",
      "788 11.9684\n",
      "789 19.6939\n",
      "790 13.2365\n",
      "791 11.18\n",
      "792 15.8604\n",
      "793 17.1099\n",
      "794 14.9842\n",
      "795 12.2046\n",
      "796 11.2178\n",
      "797 14.4205\n",
      "798 4.27089\n",
      "799 13.8325\n",
      "800 9.72865\n",
      "801 9.14545\n",
      "802 6.82988\n",
      "803 17.4943\n",
      "804 12.9296\n",
      "805 12.9872\n",
      "806 11.8348\n",
      "807 15.0544\n",
      "808 5.96759\n",
      "809 4.52243\n",
      "810 7.13266\n",
      "811 19.2523\n",
      "812 8.52572\n",
      "813 5.27483\n",
      "814 13.6568\n",
      "815 10.2932\n",
      "816 13.7032\n",
      "817 13.6211\n",
      "818 16.8058\n",
      "819 17.2485\n",
      "820 19.2736\n",
      "821 23.9088\n",
      "822 14.3237\n",
      "823 5.07413\n",
      "824 14.0755\n",
      "825 4.19699\n",
      "826 5.07343\n",
      "827 14.6823\n",
      "828 6.71452\n",
      "829 25.2913\n",
      "830 16.0951\n",
      "831 20.9233\n",
      "832 12.0859\n",
      "833 7.41825\n",
      "834 20.2341\n",
      "835 5.62908\n",
      "836 13.2562\n",
      "837 9.09108\n",
      "838 8.98616\n",
      "839 10.6414\n",
      "840 9.13204\n",
      "841 9.10432\n",
      "842 17.6935\n",
      "843 8.60269\n",
      "844 19.5675\n",
      "845 6.48078\n",
      "846 25.1087\n",
      "847 22.1726\n",
      "848 12.3086\n",
      "849 12.4919\n",
      "850 2.91671\n",
      "851 9.24805\n",
      "852 14.1183\n",
      "853 28.5853\n",
      "854 15.0882\n",
      "855 7.87638\n",
      "856 8.63201\n",
      "857 11.4793\n",
      "858 9.37569\n",
      "859 9.1788\n",
      "860 10.1932\n",
      "861 9.87413\n",
      "862 9.84113\n",
      "863 6.7504\n",
      "864 12.0833\n",
      "865 12.4649\n",
      "866 14.2861\n",
      "867 21.7642\n",
      "868 14.6033\n",
      "869 6.60989\n",
      "870 4.31728\n",
      "871 9.26325\n",
      "872 13.3199\n",
      "873 23.5184\n",
      "874 12.3374\n",
      "875 10.2755\n",
      "876 2.57336\n",
      "877 4.91201\n",
      "878 17.0041\n",
      "879 3.37475\n",
      "880 5.55463\n",
      "881 14.6686\n",
      "882 12.755\n",
      "883 5.68745\n",
      "884 3.548\n",
      "885 9.90046\n",
      "886 10.2053\n",
      "887 5.12489\n",
      "888 10.5803\n",
      "889 10.6337\n",
      "890 3.91133\n",
      "891 8.39472\n",
      "892 4.36779\n",
      "893 9.78361\n",
      "894 6.58132\n",
      "895 8.48814\n",
      "896 11.8721\n",
      "897 5.34445\n",
      "898 5.4227\n",
      "899 8.7251\n",
      "900 6.97671\n",
      "901 9.84003\n",
      "902 9.90096\n",
      "903 7.78161\n",
      "904 10.6039\n",
      "905 7.81016\n",
      "906 3.47024\n",
      "907 14.113\n",
      "908 5.72743\n",
      "909 7.60381\n",
      "910 20.4394\n",
      "911 4.36462\n",
      "912 5.79905\n",
      "913 9.85483\n",
      "914 0.0\n",
      "915 1.69197\n",
      "916 8.18301\n",
      "917 5.13257\n",
      "918 6.02342\n",
      "919 10.2929\n",
      "920 0.11236\n",
      "921 0.447978\n",
      "922 1.30601\n",
      "923 18.0085\n",
      "924 5.68041\n",
      "925 3.83041\n",
      "926 15.6159\n",
      "927 3.2409\n",
      "928 11.8369\n",
      "929 10.6711\n",
      "930 15.3423\n",
      "931 9.83711\n",
      "932 14.7603\n",
      "933 7.97392\n",
      "934 12.0036\n",
      "935 9.4608\n",
      "936 11.5605\n",
      "937 3.53842\n",
      "938 3.56181\n",
      "939 8.97118\n",
      "940 6.31618\n",
      "941 1.8828\n",
      "942 8.00009\n",
      "943 5.61958\n",
      "944 16.4153\n",
      "945 15.5689\n",
      "946 10.8314\n",
      "947 9.80649\n",
      "948 3.07956\n",
      "949 4.16428\n",
      "950 4.50383\n",
      "951 11.4649\n",
      "952 15.026\n",
      "953 7.17623\n",
      "954 5.82303\n",
      "955 6.74221\n",
      "956 9.19328\n",
      "957 4.6072\n",
      "958 8.05688\n",
      "959 6.2022\n",
      "960 16.4936\n",
      "961 6.13661\n",
      "962 4.19132\n",
      "963 12.4809\n",
      "964 12.7644\n",
      "965 9.19614\n",
      "966 3.60404\n",
      "967 10.9648\n",
      "968 13.1172\n",
      "969 15.2376\n",
      "970 2.74354\n",
      "971 10.4965\n",
      "972 9.95414\n",
      "973 8.58269\n",
      "974 14.0751\n",
      "975 6.4629\n",
      "976 9.42865\n",
      "977 10.5367\n",
      "978 7.54219\n",
      "979 9.99719\n",
      "980 16.6749\n",
      "981 7.87589\n",
      "982 8.78624\n",
      "983 4.15521\n",
      "984 9.23882\n",
      "985 11.8517\n",
      "986 5.68379\n",
      "987 17.7051\n",
      "988 5.8912\n",
      "989 8.15587\n",
      "990 5.84071\n",
      "991 25.8941\n",
      "992 10.2332\n",
      "993 17.6568\n",
      "994 7.56793\n",
      "995 7.62528\n",
      "996 6.41575\n",
      "997 12.076\n",
      "998 9.14895\n",
      "999 7.63093\n"
     ]
    }
   ],
   "source": [
    "    model = Model()\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "\n",
    "        x = tf.placeholder(shape=[None, IMAGE_SIZE, IMAGE_SIZE, 1], dtype=tf.float32, name='x')\n",
    "        y = tf.placeholder(shape=[None, NUM_LABELS], dtype=tf.float32, name='y')\n",
    "        keep_prob = tf.placeholder(tf.float32, name='dropout_prob')\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "        logits = model.inference(x, keep_prob=keep_prob)\n",
    "        loss = model.loss(logits=logits, labels=y)\n",
    "\n",
    "        accuracy = model.accuracy(logits, y)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        train_op = model.train(loss, global_step=global_step)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            writer = tf.summary.FileWriter('cache', sess.graph)\n",
    "            sess.run(init)\n",
    "            for i in range(1000):\n",
    "                offset = (i * batch_size) % (len(x_train) - batch_size)\n",
    "                batch_x, batch_y = x_train[offset:(offset + batch_size), :], y_train[offset:(offset + batch_size), :]\n",
    "\n",
    "                _, cur_loss, summary = sess.run([train_op, loss, summary_op],\n",
    "                                                feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n",
    "                writer.add_summary(summary, i)\n",
    "                print(i, cur_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, batch_size=128, learning_rate=1e-4, num_labels=10):\n",
    "        self._batch_size = batch_size\n",
    "        self._learning_rate = learning_rate\n",
    "        self._num_labels = num_labels\n",
    "\n",
    "    def inference(self, images, keep_prob):\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            kernel = self._create_weights([5, 5, 1, 32])\n",
    "            conv = self._create_conv2d(images, kernel)\n",
    "            bias = self._create_bias([32])\n",
    "            preactivation = tf.nn.bias_add(conv, bias)\n",
    "            conv1 = tf.nn.relu(preactivation, name=scope.name)\n",
    "            self._activation_summary(conv1)\n",
    "\n",
    "        # pool 1\n",
    "        h_pool1 = self._create_max_pool_2x2(conv1)\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            kernel = self._create_weights([5, 5, 32, 64])\n",
    "            conv = self._create_conv2d(h_pool1, kernel)\n",
    "            bias = self._create_bias([64])\n",
    "            preactivation = tf.nn.bias_add(conv, bias)\n",
    "            conv2 = tf.nn.relu(preactivation, name=scope.name)\n",
    "            self._activation_summary(conv2)\n",
    "\n",
    "        # pool 2\n",
    "        h_pool2 = self._create_max_pool_2x2(conv2)\n",
    "\n",
    "        with tf.variable_scope('local1') as scope:\n",
    "            reshape = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "            W_fc1 = self._create_weights([7 * 7 * 64, 1024])\n",
    "            b_fc1 = self._create_bias([1024])\n",
    "            local1 = tf.nn.relu(tf.matmul(reshape, W_fc1) + b_fc1, name=scope.name)\n",
    "            self._activation_summary(local1)\n",
    "\n",
    "        with tf.variable_scope('local2_linear') as scope:\n",
    "            W_fc2 = self._create_weights([1024, self._num_labels])\n",
    "            b_fc2 = self._create_bias([self._num_labels])\n",
    "            local1_drop = tf.nn.dropout(local1, keep_prob)\n",
    "            local2 = tf.nn.bias_add(tf.matmul(local1_drop, W_fc2), b_fc2, name=scope.name)\n",
    "            self._activation_summary(local2)\n",
    "        return local2\n",
    "\n",
    "    def train(self, loss, global_step):\n",
    "        tf.summary.scalar('learning_rate', self._learning_rate)\n",
    "        train_op = tf.train.AdamOptimizer(self._learning_rate).minimize(loss, global_step=global_step)\n",
    "        return train_op\n",
    "\n",
    "    def loss(self, logits, labels):\n",
    "        with tf.variable_scope('loss') as scope:\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "            cost = tf.reduce_mean(cross_entropy, name=scope.name)\n",
    "            tf.summary.scalar('cost', cost)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def accuracy(self, logits, y):\n",
    "        with tf.variable_scope('accuracy') as scope:\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1)), dtype=tf.float32),\n",
    "                                      name=scope.name)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "        return accuracy\n",
    "\n",
    "    def _create_conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(input=x,\n",
    "                            filter=W,\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "    def _create_max_pool_2x2(self, input):\n",
    "        return tf.nn.max_pool(value=input,\n",
    "                              ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1],\n",
    "                              padding='SAME')\n",
    "\n",
    "    def _create_weights(self, shape):\n",
    "        return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1, dtype=tf.float32))\n",
    "\n",
    "    def _create_bias(self, shape):\n",
    "        return tf.Variable(tf.constant(1., shape=shape, dtype=tf.float32))\n",
    "\n",
    "    def _activation_summary(self, x):\n",
    "        tensor_name = x.op.name\n",
    "        tf.summary.histogram(tensor_name + '/activations', x)\n",
    "        tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
