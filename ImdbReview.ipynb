{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import attrdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ImbdReview:\n",
    "    TOKEN = re.compile(r'[A-Za-z]+|[!?.:,()]')\n",
    "    \n",
    "    def __init__(self, cache_dir, file_path):\n",
    "        self._cache_dir = cache_dir\n",
    "        self._file_path = file_path\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with tarfile.open(self._file_path) as archive:\n",
    "            for filename in archive.getnames():\n",
    "                if filename.startswith('aclImdb/train/pos/'):\n",
    "                    yield self._read(archive, filename), True\n",
    "                elif filename.startswith('aclImdb/train/neg/'):\n",
    "                    yield self._read(archive, filename), False\n",
    "                    \n",
    "    def _read(self, archive, filename):\n",
    "        with archive.extractfile(filename) as file_:\n",
    "            data = file_.read().decode('utf-8')\n",
    "            data = type(self).TOKEN.findall(data)\n",
    "            data = [x.lower() for x in data]\n",
    "            return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    \n",
    "    def __init__(self,vocabulary_path, embedding_path, length):\n",
    "        self._embedding = np.load(embedding_path)\n",
    "        with bz2.open(vocabulary_path,'rt') as file_:\n",
    "            self._vocabulary = {word.strip(): i for i,word in enumerate(file_)}\n",
    "        self._length = length\n",
    "        \n",
    "    def __call__(self, sequence):\n",
    "        data=np.zeros([self._length, self._embedding.shape[1]])\n",
    "        indices = [self._vocabulary.get(x,0) for x in sequence]\n",
    "        embedded = self._embedding[indices]\n",
    "        data[:len(sequence)] = embedded\n",
    "        return data\n",
    "    \n",
    "    def dimensions(self):\n",
    "        return self._embedding.shape[1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SequenceClassificationModel:\n",
    "    \n",
    "    def __init__(self,data, target, params):\n",
    "        self.data=data\n",
    "        self.target=target\n",
    "        self.params=params\n",
    "        self.prediction\n",
    "        self.cost\n",
    "        self.error\n",
    "        self.optimize\n",
    "        \n",
    "    def length(self):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(self.data), axis=2))\n",
    "        length = tf.reduce_sum(used, axis=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    def prediction(self):\n",
    "        output, _ = tf.nn.dynamic_rnn(self.params.rnn_cell(self.params.rnn_hidden), self.data, dtype=tf.float32, sequence_length=self.length)\n",
    "        \n",
    "        last = self._last_relevant(output, self.length)\n",
    "        num_classes = int(self.target.get_shape()[1])\n",
    "        weight = tf.Variable(tf.truncated_normal([self.params.rnn_hidden, num_classes],stddev=0.1))\n",
    "        bias = tf.Variable(tf.constant(0.1,[num_classes]))\n",
    "        prediction = tf.nn.softmax(tf.matmul(last, weight), bias)\n",
    "        return prediction\n",
    "    \n",
    "    # flatten last from sequences x time_steps x word_vectors \n",
    "    def _last_relevant(output, length):\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        max_length = int(output.get_shape()[1])\n",
    "        output_size = int(output.get_shape()[2])\n",
    "        index = tf.range(0,batch_size) * max_length + (length-1)\n",
    "        flat = tf.reshape(output, [-1, output_size])\n",
    "        relevant = tf.gather(flat, index)\n",
    "        return relevant\n",
    "    \n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "    \n",
    "    def optimize(self):\n",
    "        gradient = self.params.optimizer.compute_gradient(self.cost)\n",
    "        # modify the gradients\n",
    "        if self.params.gradient_clipping:\n",
    "            limit = self.params.gradient_clipping\n",
    "            gradient = [(tf.clip_by_value(g, -limit, limit),v) if g is not None else (None, v) for g,v in gradient]\n",
    "            \n",
    "            optimize = self.params.optimizer.apply_gradients(gradient)\n",
    "            \n",
    "        return optimize\n",
    "    \n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(tf.argmax(self.target,1),tf.argmax(self.prediction,1))\n",
    "        return tf.reduce_mean(tf.cast(mistakes), tf.float32)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_batched(iterator, length, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        data = np.zeros((batch_size, length, embedding.dimensions))\n",
    "        target = np.zeros((batch_size, 2))\n",
    "        for index in range(batch_size):\n",
    "            text, label = next(iterator)\n",
    "            data[index] = embedding(text)\n",
    "            target[index] = [1, 0] if label else [0, 1]\n",
    "        yield data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = attrdict.AttrDict(\n",
    "rnn_cell=tf.contrib.rnn.GRUCell,\n",
    "rnn_hidden=300,\n",
    "optimizer=tf.train.RMSPropOptimizer(0.002),\n",
    "batch_size=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = ImbdReview(cache_dir='ImdbReview',file_path='aclImdb.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length = max(len(x[0]) for x in reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ImdbReview/embedding.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-74fbbf5fef3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ImdbReview/vocabulary.bz2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ImdbReview/embedding.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-7197ecbafdbd>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocabulary_path, embedding_path, length)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocabulary_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mbz2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lei\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ImdbReview/embedding.npy'"
     ]
    }
   ],
   "source": [
    "embedding = Embedding('ImdbReview/vocabulary.bz2','ImdbReview/embedding.npy', length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-47-64e4f4178bc0>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-47-64e4f4178bc0>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    feed = {data: batch[0], target: batch[1]}\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batches = preprocess_batched(reviews, length, embedding, params.batch_size)\n",
    "data = tf.placeholder(tf.float32, [None, length, embedding.dimensions])\n",
    "target = tf.placeholder(tf.float32, [None, 2])\n",
    "model = SequenceClassificationModel(data, target, params)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for index, batch in enumerate(batches):\n",
    "feed = {data: batch[0], target: batch[1]}\n",
    "error, _ = sess.run([model.error, model.optimize], feed)\n",
    "print('{}: {:3.1f}%'.format(index + 1, 100 * error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2758"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
